{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normalizer = Normalizer()\n",
    "encoder = OneHotEncoder()\n",
    "C1 = np.sqrt(2)\n",
    "K2 = 10\n",
    "K = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self):\n",
    "        self.Z = self.X@self.W\n",
    "\n",
    "    def backward(self):\n",
    "        self.dZ_dW = (self.X).T  \n",
    "        self.dZ_dO_prev = self.W  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias : \n",
    "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
    "        self.B = bias\n",
    "        self.Z = Z\n",
    "\n",
    "    def forward(self):\n",
    "        self.Z = self.Z + self.B\n",
    "    \n",
    "    def backward(self):\n",
    "        self.dZ_dB = np.identity( self.B.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation : \n",
    "    def __init__(self, act = \"linear\", Z = np.random.random((1,1))):\n",
    "        self.Z = Z\n",
    "        self.act =  act\n",
    "        # self.O\n",
    "    \n",
    "    def forward(self,):\n",
    "        if(self.act == 'linear'):\n",
    "            self.O = self.Z\n",
    "        elif(self.act == 'sigmoid'):\n",
    "            self.O = self.sigmoid(self.Z)\n",
    "        elif(self.act == 'softmax'):\n",
    "            self.O = self.softmax(self.Z)\n",
    "        elif(self.act == 'tanh'):\n",
    "            self.O = np.tanh(self.Z)\n",
    "    def backward(self,):\n",
    "        if(self.act == 'linear'):\n",
    "            self.dO_dZ = np.identity( self.Z.shape[1] )\n",
    "        elif(self.act == 'sigmoid'):\n",
    "            diag_entries = np.multiply(self.O, 1-self.O).reshape(-1)\n",
    "            self.dO_dZ = np.diag(diag_entries)\n",
    "        elif(self.act == 'softmax'):\n",
    "            self.dO_dZ = np.diag( self.O.reshape(-1) ) - (self.O.T)@( (self.O))  # Shape = (K,K) where K = len( sZ )\n",
    "        elif(self.act == 'tanh'):\n",
    "            self.dO_dZ = np.diag(1 - self.O.reshape(-1)**2)\n",
    "    def sigmoid(self, Z: np.ndarray):\n",
    "        return 1./(1 + np.exp(-Z))\n",
    "    def softmax(self, Z : np.ndarray):\n",
    "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
    "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, Y , Ypred, lossType = \"mse\",):\n",
    "        self.Y = Y\n",
    "        self.O = Ypred\n",
    "        self.lossType = lossType\n",
    "    def forward(self):\n",
    "        if(self.lossType == \"mse\"):\n",
    "            self.L = np.mean((self.O - self.Y)**2)\n",
    "        elif(self.lossType == \"ce\"):\n",
    "            self.L = - np.sum(self.Y * np.log(self.O+(1e-40)))\n",
    "    def backward(self):\n",
    "        if(self.lossType == \"mse\"):\n",
    "            self.dL_dO = (2/len(self.Y))*(self.O - self.Y).T\n",
    "        elif(self.lossType == \"ce\"):\n",
    "            self.dL_dO = -1*(self.Y/(self.O + 1e-40)).T  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, numNeuronsPrev, numNeurons, activation_name=\"linear\", seed=K):\n",
    "        np.random.seed(seed)  # for reproducability of code\n",
    "\n",
    "        self.W = np.random.random((numNeuronsPrev, numNeurons)) *  C1/(2*(numNeuronsPrev + numNeurons))\n",
    "        self.B = np.random.random((1, numNeurons))* C1 / (2*numNeurons)\n",
    "        self.numNeuronsPrev = numNeuronsPrev\n",
    "        self.numNeurons = numNeurons\n",
    "        self.X = np.random.random((1,numNeuronsPrev))   \n",
    "        self.Z = np.random.random((1, numNeurons))  \n",
    "        \n",
    "        if activation_name == 'linear':\n",
    "            self.layerActivation = Activation(\"linear\",self.Z)\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.layerActivation = Activation(\"sigmoid\",self.Z)\n",
    "        elif activation_name == 'softmax':\n",
    "            self.layerActivation = Activation(\"softmax\",self.Z)\n",
    "        elif activation_name == 'tanh':\n",
    "            self.layerActivation = Activation(\"tanh\",self.Z)\n",
    "\n",
    "        # define multiplication layer, bias addition layer , and activation layer\n",
    "        self.ProductWX = MatMul(self.X, self.W)\n",
    "        self.AddBtoWX = AddBias(self.B, self.B)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.ProductWX.X = self.X\n",
    "        self.ProductWX.forward()\n",
    "\n",
    "        self.AddBtoWX.Z = self.ProductWX.Z\n",
    "        self.AddBtoWX.forward()\n",
    "\n",
    "        self.layerActivation.Z = self.AddBtoWX.Z\n",
    "        self.layerActivation.forward()\n",
    "        self.Z = self.layerActivation.O\n",
    "\n",
    "    def backward(self):\n",
    "        self.layerActivation.backward()\n",
    "        self.AddBtoWX.backward()\n",
    "        self.ProductWX.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(Layer):\n",
    "\n",
    "    def __init__(self, layers, learning_rate=0.01, loss=\"mse\", seed=K):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.listLayers = layers\n",
    "\n",
    "        self.size_inp = self.listLayers[0].X.shape\n",
    "        self.size_out = self.listLayers[-1].Z.shape\n",
    "        \n",
    "        self.num_layers = len(self.listLayers)  \n",
    "        self.alpha = learning_rate\n",
    "\n",
    "\n",
    "        self.X = np.random.random(self.size_inp)   \n",
    "        self.Y = np.random.random(self.size_out) \n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.loss = Loss(self.Y, self.Y, \"mse\")\n",
    "        if loss == \"ce\":\n",
    "            self.loss = Loss(self.Y, self.Y, \"ce\")\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.listLayers[0].X = self.X\n",
    "        self.loss.Y = self.Y\n",
    "\n",
    "        self.listLayers[0].forward()\n",
    "        for i in range(1, self.num_layers):\n",
    "            self.listLayers[i].X = self.listLayers[i-1].Z\n",
    "            self.listLayers[i].forward()\n",
    "\n",
    "        self.loss.O = self.listLayers[-1].Z\n",
    "        self.loss.forward()\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        self.loss.Z = self.Y\n",
    "        self.loss.backward()\n",
    "        self.grad_nn = self.loss.dL_dO\n",
    "        for i in range(self.num_layers-1, -1, -1):\n",
    "            self.listLayers[i].backward()\n",
    "\n",
    "            dL_dZ = np.dot(\n",
    "                self.listLayers[i].layerActivation.dO_dZ, self.grad_nn)\n",
    "            dL_dW = np.dot(self.listLayers[i].ProductWX.dZ_dW, dL_dZ.T)\n",
    "            dL_dB = np.dot(self.listLayers[i].AddBtoWX.dZ_dB, dL_dZ).T\n",
    "\n",
    "            # Update W & B\n",
    "            self.listLayers[i].W -= self.alpha*dL_dW\n",
    "            self.listLayers[i].B -= self.alpha*dL_dB\n",
    "\n",
    "            # Update outer_grad\n",
    "            self.grad_nn = np.dot(\n",
    "                self.listLayers[i].ProductWX.dZ_dO_prev, dL_dZ)\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_network(inp_shape, layers_sizes, layers_activations):\n",
    "#     layers = []\n",
    "#     n_layers = len(layers_sizes)\n",
    "#     layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0])\n",
    "#     layers.append(layer_0)\n",
    "#     inp_shape_next = layers_sizes[0]\n",
    "#     for i in range(1, n_layers):\n",
    "#         layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i])\n",
    "#         layers.append(layer_i)\n",
    "#         inp_shape_next = layers_sizes[i]\n",
    "\n",
    "#     out_shape = inp_shape_next\n",
    "#     return inp_shape, out_shape, layers\n",
    "def create_network(input_shape, layer_sizes, layer_activations):\n",
    "    network_layers = []\n",
    "    num_layers = len(layer_sizes)\n",
    "\n",
    "    # Create the first layer\n",
    "    first_layer = Layer(input_shape, layer_sizes[0], layer_activations[0])\n",
    "    network_layers.append(first_layer)\n",
    "    next_layer_input_shape = layer_sizes[0]\n",
    "\n",
    "    # Create subsequent layers\n",
    "    for i in range(1, num_layers):\n",
    "        current_layer = Layer(next_layer_input_shape, layer_sizes[i], layer_activations[i])\n",
    "        network_layers.append(current_layer)\n",
    "        next_layer_input_shape = layer_sizes[i]\n",
    "\n",
    "    output_shape = next_layer_input_shape\n",
    "    return input_shape, output_shape, network_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(X_train,\n",
    "                      y_train,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      nn,\n",
    "                      inp_shape=1,   # dimension of input\n",
    "                      out_shape=1,   # dimension of output\n",
    "                      n_iterations=1000,\n",
    "                      task=\"regression\"  # [ \"regression\", \"classification\"]\n",
    "                      ):\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
    "        nn.X = X_sample\n",
    "        nn.Y = Y_sample\n",
    "\n",
    "        nn.forward()  # Forward Pass\n",
    "        nn.backward()  # Backward Pass\n",
    "        # print(\"Loss is \", nn.loss.L)\n",
    "\n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "\n",
    "    if nn.loss.lossType == \"mse\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        train_error = nn.loss.L\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "\n",
    "        nn.forward()\n",
    "\n",
    "        test_error = nn.loss.L\n",
    "\n",
    "        print(\"Mean Squared Loss Error (Train Data)  : %0.5f\" % train_error)\n",
    "        print(\"Mean Squared Loss Error (Test Data)  : %0.5f\" % test_error)\n",
    "\n",
    "    if nn.loss.lossType == \"ce\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        print(nn.loss.L)\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        y_pred = np.argmax(nn.loss.O, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred = np.argmax(nn.loss.O, axis=1)\n",
    "        acc = 1*(y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.2]\n",
      " [28. ]\n",
      " [22.5]\n",
      " [32.7]\n",
      " [24.5]\n",
      " [14.4]\n",
      " [18.6]\n",
      " [31.5]\n",
      " [24. ]\n",
      " [23. ]\n",
      " [22.2]\n",
      " [23.9]\n",
      " [32.2]\n",
      " [13.9]\n",
      " [10.8]\n",
      " [15.3]\n",
      " [22.6]\n",
      " [14.5]\n",
      " [24.1]\n",
      " [32. ]\n",
      " [19.9]\n",
      " [18.3]\n",
      " [44.8]\n",
      " [17.6]\n",
      " [29. ]\n",
      " [27.5]\n",
      " [24.6]\n",
      " [12.7]\n",
      " [20.4]\n",
      " [19.6]\n",
      " [43.8]\n",
      " [16.5]\n",
      " [13. ]\n",
      " [20.9]\n",
      " [41.3]\n",
      " [20.8]\n",
      " [18.7]\n",
      " [19.5]\n",
      " [20.6]\n",
      " [20.6]\n",
      " [14.6]\n",
      " [13.8]\n",
      " [19.3]\n",
      " [24.8]\n",
      " [37.9]\n",
      " [50. ]\n",
      " [23.3]\n",
      " [23.1]\n",
      " [16.1]\n",
      " [37.3]\n",
      " [ 9.6]\n",
      " [42.8]\n",
      " [ 8.1]\n",
      " [21.9]\n",
      " [14.6]\n",
      " [25.1]\n",
      " [23.4]\n",
      " [10.2]\n",
      " [24.6]\n",
      " [24.3]\n",
      " [33.8]\n",
      " [31.2]\n",
      " [28.4]\n",
      " [19.5]\n",
      " [19.8]\n",
      " [11.9]\n",
      " [32.9]\n",
      " [31.6]\n",
      " [17.8]\n",
      " [19.9]\n",
      " [43.5]\n",
      " [50. ]\n",
      " [ 7.2]\n",
      " [ 9.5]\n",
      " [15.6]\n",
      " [33.3]\n",
      " [17.2]\n",
      " [28.7]\n",
      " [22.9]\n",
      " [19.4]\n",
      " [29.6]\n",
      " [24. ]\n",
      " [37. ]\n",
      " [23.6]\n",
      " [22.2]\n",
      " [23.9]\n",
      " [21.1]\n",
      " [22.6]\n",
      " [18.5]\n",
      " [34.9]\n",
      " [19.6]\n",
      " [50. ]\n",
      " [22.4]\n",
      " [17.8]\n",
      " [33.4]\n",
      " [26.6]\n",
      " [23.2]\n",
      " [18.7]\n",
      " [20.1]\n",
      " [13.1]\n",
      " [12.8]\n",
      " [13.4]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "raw_df = pd.read_csv(\"http://lib.stat.cmu.edu/datasets/boston\", sep=\"\\s+\", skiprows=22, header=None) \n",
    "data = {'data': np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]), 'target': raw_df.values[1::2, 2]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalizer.fit_transform(data['data']), data['target'].reshape(-1, 1), test_size=0.2)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 59.95095\n",
      "Mean Squared Loss Error (Test Data)  : 65.52835\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [1]\n",
    "layers_activations = ['linear']\n",
    "\n",
    "inp_shape, out_shape, layers = create_network(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mse'\n",
    "\n",
    "nn = NeuralNetwork(layers, 1e-1, loss_nn)\n",
    "\n",
    "trainNN(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 95.01540\n",
      "Mean Squared Loss Error (Test Data)  : 87.94494\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,1]\n",
    "layers_activations = ['sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = create_network(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mse'\n",
    "\n",
    "nn = NeuralNetwork(layers, 1e-2, loss_nn)\n",
    "\n",
    "trainNN(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss Error (Train Data)  : 113.99156\n",
      "Mean Squared Loss Error (Test Data)  : 103.97630\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,13,1]\n",
    "layers_activations = ['sigmoid','sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = create_network(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mse'\n",
    "\n",
    "nn = NeuralNetwork(layers, 1e-2, loss_nn)\n",
    "\n",
    "trainNN(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "data['data'] = 1*(data['data'] >= 10)\n",
    "X_train, X_test, y_train, y_test = train_test_split( data['data'] , encoder.fit_transform(data['target'].reshape(-1, 1)).toarray(), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(360, 64)\n",
      "(1437, 10)\n",
      "(360, 10)\n",
      "Mean Squared Loss Error (Train Data)  : 0.01024\n",
      "Mean Squared Loss Error (Test Data)  : 0.01658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','sigmoid']\n",
    "\n",
    "inp_shape, out_shape, layers = create_network(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mse'\n",
    "\n",
    "nn = NeuralNetwork(layers, 1e-1, loss_nn)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "trainNN(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358.6386777881987\n",
      "Classification Accuracy (Training Data ): 1330/1437 = 92.55393180236604 %\n",
      "Classification Accuracy (Testing Data ): 326/360 = 90.55555555555556 %\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [89,10]\n",
    "layers_activations = ['tanh','softmax']\n",
    "\n",
    "inp_shape, out_shape, layers = create_network(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'ce'\n",
    "\n",
    "nn = NeuralNetwork(layers, 1e-2, loss_nn)\n",
    "\n",
    "trainNN(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 inp_shape,\n",
    "                 activation='tanh',\n",
    "                 lr=0.01,\n",
    "                 size_output=1,\n",
    "                 seed=K2):                                                \n",
    "\n",
    "        np.random.seed(seed)        \n",
    "\n",
    "        self.inp = np.random.rand(*inp_shape)\n",
    "        self.inp_shape = inp_shape\n",
    "        # number of channels in input here denoted as inp\n",
    "        self.size_input = self.inp.shape[0]\n",
    "        # number of output channels\n",
    "        self.size_output = size_output\n",
    "        self.filters_shape = (self.size_output, self.size_input,  3, 3)\n",
    "        self.out_shape = (\n",
    "            self.size_output, self.inp.shape[1] - 3 + 1, self.inp.shape[2] -3 + 1)\n",
    "        self.flatten_shape = self.out_shape[0] * \\\n",
    "            self.out_shape[1]*self.out_shape[2]\n",
    "        self.lr = lr\n",
    "\n",
    "        # Randomly initialize filters, biases, output, flatten output\n",
    "        self.filters = np.random.rand(*self.filters_shape)\n",
    "        self.biases = np.random.rand(*self.out_shape)\n",
    "        self.out = np.random.rand(*self.out_shape)\n",
    "        self.flatten_out = np.random.rand(1, self.flatten_shape)\n",
    "\n",
    "        # Define activation function\n",
    "        if activation == 'tanh':\n",
    "            self.activation_layer = Activation(\"tanh\",self.out)\n",
    "            \n",
    "    def rotateMatrix(self, mat):\n",
    "        out = np.zeros(mat.shape)\n",
    "        N = mat.shape[0]\n",
    "        M = mat.shape[1]\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                out[i, N-1-j] = mat[M-1-i, j]\n",
    "        return out\n",
    "\n",
    "    def forward(self, ):\n",
    "        self.out = np.copy(self.biases)  # add bias to output\n",
    "        for i in range(self.size_output):\n",
    "            for j in range(self.size_input):\n",
    "                self.out[i] += self.convolve(self.inp[j], self.filters[i, j])\n",
    "\n",
    "        self.flatten()\n",
    "        self.activation_layer.Z = self.flatten_out\n",
    "        self.activation_layer.forward()\n",
    "    \n",
    "\n",
    "\n",
    "    def backward(self, grad_nn):\n",
    "\n",
    "        self.activation_layer.backward()\n",
    "        loss_gradient = np.dot(self.activation_layer.dO_dZ, grad_nn)\n",
    "        # reshape to (size_output, H_out, W_out)\n",
    "        loss_gradient = np.reshape(loss_gradient, self.out_shape)\n",
    "\n",
    "        # dL/dKij for each filter  Kij    1<=i<=size_input , 1<=j<=size_output\n",
    "        self.filters_gradient = np.zeros(self.filters_shape)\n",
    "        self.input_gradient = np.zeros(self.inp_shape)  # dL/dXj\n",
    "        self.biases_gradient = loss_gradient  # dL/dBi  = dL/dYi\n",
    "        padded_loss_gradient = np.pad(loss_gradient, ((\n",
    "            0, 0), (self.filters_shape[2]-1, self.filters_shape[2]-1), (self.filters_shape[3]-1, self.filters_shape[3]-1)))\n",
    "\n",
    "        for i in range(self.size_output):\n",
    "            for j in range(self.size_input):\n",
    "                self.filters_gradient[i, j] = self.convolve(self.inp[j], loss_gradient[i]) \n",
    "                rot180_Kij = self.rotateMatrix(self.filters[i, j]) # np.rot90(np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))\n",
    "                self.input_gradient[j] += self.convolve(padded_loss_gradient[i], rot180_Kij)\n",
    "        \n",
    "\n",
    "        # update filters and biases\n",
    "        self.filters -= self.lr*self.filters_gradient\n",
    "        self.biases -= self.lr*self.biases_gradient\n",
    "\n",
    "    # flattening output to 1 Dimension so it can be fed int neural network\n",
    "\n",
    "    def flatten(self, ):\n",
    "        self.flatten_out = self.out.reshape(1, -1)\n",
    "\n",
    "    # convolutional operation with stride=1\n",
    "    def convolve(self, mat1, mat2, stride=1, pad=0):\n",
    "        output1 = (mat1.shape[0] + 2*pad - mat2.shape[0])//stride + 1\n",
    "        output2 = (mat1.shape[1] + 2*pad - mat2.shape[1])//stride + 1\n",
    "        output_size = (output1, output2)\n",
    "\n",
    "        mat1=np.pad(mat1, pad)\n",
    "        output = np.zeros(output_size)\n",
    "\n",
    "        for i in range(0, output1, stride):\n",
    "            for j in range(0, output2, stride):\n",
    "                for k in range(mat2.shape[0]):\n",
    "                    for l in range(mat2.shape[1]):\n",
    "                        output[i][j]+=mat1[i+k][j+l]*mat2[k][l]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(convolutional_layer, network, X, Y):\n",
    "    # forward pass of convolutional layer \n",
    "    convolutional_layer.inp = X \n",
    "    convolutional_layer.forward()\n",
    "\n",
    "    # forward pass of neural network \n",
    "    network.X = convolutional_layer.activation_layer.O\n",
    "    network.Y = Y \n",
    "    network.forward()  \n",
    "    \n",
    "def backward(network, convolutional_layer): \n",
    "    network.backward() \n",
    "    convolutional_layer.backward( network.grad_nn )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCNN(X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test, #cnn,\n",
    "            convolutional_layer,\n",
    "            network,\n",
    "            inp_shape,\n",
    "            out_shape,\n",
    "            n_iterations=5000):\n",
    "\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(out_shape)\n",
    "\n",
    "        X = X_sample\n",
    "        Y = Y_sample\n",
    "        # cnn.X = X_sample\n",
    "        # cnn.Y = Y_sample\n",
    "\n",
    "        forward(convolutional_layer, network, X, Y)  # Forward Pass\n",
    "        # cnn.forward()\n",
    "        backward(network, convolutional_layer)  # Backward Pass\n",
    "        # cnn.backward()\n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "    X_train = X_train.reshape(-1, 8, 8)\n",
    "    y_true = np.argmax(y_train, axis=1)\n",
    "    acc = 0\n",
    "    for i in range(len(X_train)):\n",
    "        X = X_train[i][np.newaxis, :, :]\n",
    "        Y = y_train[i]\n",
    "        forward(convolutional_layer, network, X, Y)\n",
    "        y_pred_i = np.argmax(network.loss.O, axis=1)\n",
    "        if (y_pred_i == y_true[i]):\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"Classification Accuracy (Training Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n",
    "\n",
    "    X_test = X_test.reshape(-1, 8, 8)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    acc = 0\n",
    "    for i in range(len(X_test)):\n",
    "        X = X_test[i][np.newaxis, :, :]\n",
    "        Y = y_test[i]\n",
    "        forward(convolutional_layer, network, X, Y)\n",
    "        y_pred_i = np.argmax(network.loss.O, axis=1)\n",
    "        if (y_pred_i == y_true[i]):\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"Classification Accuracy (Testing Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "data['data'] = 1*(data['data'] >= 8)\n",
    "X_train, X_test, y_train, y_test = train_test_split( data['data'] , encoder.fit_transform(data['target'].reshape(-1, 1)).toarray(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Training Data ):1262/1437 = 87.82185107863604 %\n",
      "Classification Accuracy (Testing Data ):312/360 = 86.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "input_size = (1,8,8)   # sklearn digit dataset has images of shape 1 x 8 x 8\n",
    "output_size = 16  # 16 channel output \n",
    "conv_activation = 'tanh'\n",
    "convolutional_layer = ConvolutionalLayer(input_size, \n",
    "                                        size_output = output_size,\n",
    "                                        activation = conv_activation,\n",
    "                                        lr = 0.01)\n",
    "nn_inp_shape = convolutional_layer.flatten_shape \n",
    "layers_sizes = [10]\n",
    "layers_activations = ['softmax']\n",
    "\n",
    "_, _, layers = create_network(nn_inp_shape, layers_sizes, layers_activations)\n",
    "\n",
    "network = NeuralNetwork(layers, 1e-2, 'ce')\n",
    "\n",
    "out_shape =  (1, layers_sizes[-1])  \n",
    "trainCNN(X_train,y_train,X_test,y_test,convolutional_layer, network,input_size, out_shape,n_iterations=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code to implement after training\n",
    "\n",
    "fig,axs = plt.subplots(2,2)\n",
    "for i in range(4):\n",
    "    l=i//2\n",
    "    j=i%2\n",
    "    k = np.random.randint(len(X_test))\n",
    "    O = X_test[k].reshape(1, 8, 8)\n",
    "    network.X = O\n",
    "    forward(convolutional_layer, network, O, 1)\n",
    "    mnist_array = O[0]\n",
    "    axs[l, j].imshow(mnist_array, cmap='gray', interpolation='nearest')\n",
    "    axs[l, j].axis('off')\n",
    "    axs[l, j].set_title(f\"Prediction = {np.argmax(network.listLayers[-1].Z)}\" )\n",
    "\n",
    "plt.show()\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
